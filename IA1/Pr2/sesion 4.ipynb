{"cells":[{"cell_type":"markdown","id":"6fb9aff7","metadata":{"id":"6fb9aff7"},"source":["# Práctica 2. Aprendizaje por refuerzo\n","## Inteligencia Artificial I    2021/2022"]},{"cell_type":"markdown","id":"11ae0570","metadata":{"id":"11ae0570"},"source":["## Parte 1. Taxi\n","En este ejemplo (visto en clase) hay 4 ubicaciones (etiquetadas con letras diferentes) y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra. Se trata de llevar al pasajero que inicialmente está en (Y) a la posición destino (R). El taxi solo puede coger y dejar pasajeros en las posiciones marcadas.\n","Recibimos +20 puntos por dejar al pasajero con éxito y perderemos 1 punto por cada paso de tiempo. También hay una penalización de 10 puntos por acciones ilegales de recoger y dejar.\n","\n","Vamos a utilizar OpenAI's Gym en Python donde tenemos definido este entorno y podemos desarrollar nuestro agente y evaluarlo. gym nos proporciona la representación y la visualización del tablero (render) por lo que no tenemos que hacerlo (aunque sería sencillo representarlo y resolverlo con cualquier algoritmo de búsqueda en AIMA). "]},{"cell_type":"code","execution_count":null,"id":"09e4c62a","metadata":{"id":"09e4c62a","outputId":"6bee4790-5a7c-43b1-d8bf-34c85c2f1ab0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: cmake in c:\\users\\app2m\\anaconda3\\lib\\site-packages (3.21.4)\n","Requirement already satisfied: gym in c:\\users\\app2m\\anaconda3\\lib\\site-packages (0.21.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\app2m\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n","Requirement already satisfied: numpy>=1.18.0 in c:\\users\\app2m\\anaconda3\\lib\\site-packages (from gym) (1.20.1)\n","Requirement already satisfied: scipy in c:\\users\\app2m\\anaconda3\\lib\\site-packages (1.6.2)\n","Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\app2m\\anaconda3\\lib\\site-packages (from scipy) (1.20.1)\n"]}],"source":["# Para el ejemplo del taxi y el cart pool es necesario instalar algunas librerías (gym,cmake,scipy) si no están ya instaladas\n","import sys\n","!{sys.executable} -m pip install cmake\n","# Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.\n","# La librería gym proporciona problemas de prueba — environments — con una interfaz general que puedes usar para probar \n","# distintos algoritmos y configuraciones de RL. \n","\n","!{sys.executable} -m pip install gym\n","#tps://gym.openai.com/docs/\n","\n","!{sys.executable} -m pip install scipy"]},{"cell_type":"markdown","id":"9dcb01ea","metadata":{"id":"9dcb01ea"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"id":"fa8045bd","metadata":{"id":"fa8045bd","outputId":"008b8252-b8c1-44d4-b351-2f8bb9dc9a2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","|\u001b[43m \u001b[0m: : : : |\n","| | : | : |\n","|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","\n"]}],"source":["import gym\n","# el ejemplo del taxi es un entorno que ya está definido en gym por lo que no tenemos que representar este problema.\n","env = gym.make(\"Taxi-v3\").env\n","env.render()"]},{"cell_type":"markdown","id":"60e082f7","metadata":{"id":"60e082f7"},"source":["La interfaz principal del entorno gym es env. \n","Vamos a utilizar los siguientes métodos de env:\n","\n","- env.reset: restablece el entorno y devuelve un estado inicial aleatorio.\n","\n","- env.step (acción): realiza un paso en el entorno. Devuelve:\n","\n","      observación: Observaciones del entorno\n","      recompensa: si su acción fue beneficiosa o no\n","      done: Indica si hemos recogido y dejado a un pasajero (fin de un episodio)\n","      info: información adicional como rendimiento y latencia para depuración\n","      \n","- env.render: renderiza el entorno (útil para visualizar el entorno)"]},{"cell_type":"code","execution_count":null,"id":"a9efd3a4","metadata":{"id":"a9efd3a4","outputId":"f210741f-e42e-4785-a3c0-91a58e157baa"},"outputs":[{"data":{"text/plain":["266"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["env.reset() # resetea el estado del problema a un estado aleatorio. "]},{"cell_type":"code","execution_count":null,"id":"b957606d","metadata":{"id":"b957606d","outputId":"fdce136b-670e-41b4-c72b-0c1e3f2287c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|R: | : :\u001b[34;1mG\u001b[0m|\n","| : | : : |\n","| : : :\u001b[43m \u001b[0m: |\n","| | : | : |\n","|\u001b[35mY\u001b[0m| : |B: |\n","+---------+\n","\n"]}],"source":["env.render() #visualiza el estado del problema"]},{"cell_type":"markdown","id":"867bf99e","metadata":{"id":"867bf99e"},"source":["- El cuadrado representa el taxi, que es amarillo sin pasajero y verde con pasajero.\n","- La marca (\"|\") representa una pared que el taxi no puede cruzar.\n","- R, G, Y, B son las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros y la letra morada es el destino actual.\n"]},{"cell_type":"code","execution_count":null,"id":"965936a4","metadata":{"id":"965936a4","outputId":"c92cd114-0248-44b0-a9fd-e048da77fe71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action Space Discrete(6)\n","State Space Discrete(500)\n"]}],"source":["print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"]},{"cell_type":"markdown","id":"70fb3770","metadata":{"id":"70fb3770"},"source":["El Action Space tiene tamaño 6 y el State Space tiene tamaño 500. El algoritmo RL no necesita más información que estas dos cosas. Todo lo que necesitamos es una forma de identificar un estado de forma única asignando un número único a cada estado posible, y RL aprende a elegir un número de acción de 0 a 5 donde:\n","   - 0 = sur\n","   - 1 = norte\n","   - 2 = este\n","   - 3 = oeste\n","   - 4 = recoger\n","   - 5 = dejar\n","\n","Los 500 estados corresponden a una codificación de la ubicación del taxi, la ubicación del pasajero y la ubicación de destino.\n","\n","El aprendizaje por refuerzo aprenderá un mapeo de estados con la acción óptima a realizar en ese estado. Para ello realizaremos un proceso de exploración, es decir, el agente explora el entorno y toma acciones basadas en las recompensas definidas en el entorno. La acción óptima para cada estado es la acción que tiene la mayor recompensa acumulativa a largo plazo según la fórmula vista para el Q-learning.\n","\n","Dado el estado representado en la imagen anterior, vamos a codificar su estado y dárselo al entorno para que se renderice en Gym. \n","Las filas y columnas se numeran de 0 a 4 y, como se ve en la imagen, tenemos el taxi en la fila 3, columna 1, nuestro pasajero está en la ubicación 2 y nuestro destino es la ubicación 0:  R (0), G (1), Y (2), B (3)\n","\n","Usando el método de codificación de estado Taxi-v3, podemos hacer lo siguiente:"]},{"cell_type":"code","execution_count":null,"id":"f1e7a817","metadata":{"id":"f1e7a817","outputId":"f63d86c2-4033-4016-c4db-c7920a1abbd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["State: 328\n","+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| |\u001b[43m \u001b[0m: | : |\n","|\u001b[34;1mY\u001b[0m| : |B: |\n","+---------+\n","\n"]}],"source":["state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n","print(\"State:\", state)\n","env.s = state\n","env.render()\n"]},{"cell_type":"markdown","id":"6f90a9a0","metadata":{"id":"6f90a9a0"},"source":["Podemos asertar las posiciones en el estado con env.encode() o usar los números entre 0 y 499 que son los 500 estados válidos en el espacio de estados. "]},{"cell_type":"code","execution_count":null,"id":"d247993f","metadata":{"id":"d247993f","outputId":"4dc59eaf-d072-4146-e6c0-c406db7f6b45"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n","+---------+\n","\n"]}],"source":["env.s = 499\n","env.render()"]},{"cell_type":"code","execution_count":null,"id":"deec95f7","metadata":{"id":"deec95f7","outputId":"299d0224-7450-4344-d0bf-268cbef6aeb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |B: |\n","+---------+\n","\n"]}],"source":["env.s = 0\n","env.render()"]},{"cell_type":"code","execution_count":null,"id":"c772c7de","metadata":{"id":"c772c7de","outputId":"93589d29-0b4e-4524-9a31-4b55d034c764"},"outputs":[{"data":{"text/plain":["{0: [(1.0, 428, -1, False)],\n"," 1: [(1.0, 228, -1, False)],\n"," 2: [(1.0, 348, -1, False)],\n"," 3: [(1.0, 328, -1, False)],\n"," 4: [(1.0, 328, -10, False)],\n"," 5: [(1.0, 328, -10, False)]}"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["# Con el diccionario P del entorno (tiene más informacion que la matriz R que hemos visto en clase) podemos ver los valores de reward asignados por defecto al estado 328 que usamos como ejemplo\n","env.P[328]"]},{"cell_type":"markdown","id":"b32eade9","metadata":{"id":"b32eade9"},"source":["Este diccionario tiene la siguiente estructura {acción: [(probabilidad, próximo estado, recompensa, hecho)]}.\n","\n","- El valor de 0-5 corresponde a las acciones (sur, norte, este, oeste, recogida, entrega) que el taxi puede realizar.\n","- La probabilidad es siempre 1.0 (en este entorno)\n","- nextstate muestra el estado siguiente usando la acción en este índice del diccionario.\n","- Las acciones de movimiento tienen por defecto una recompensa de -1 y pickup / dropoff tiene -10 Si estuvimos en un estado donde el taxi tiene un pasajero y estamos en la posición destino correcta se vería una recompensa de 20 en la action dropoff (5 ))\n","- done se utiliza para indicarnos cuándo hemos dejado a un pasajero en el lugar correcto. Cada dejada de un pasajero (dropoff) con éxito es el final de un episodio.\n","\n","Hay que tener en cuenta que si nuestro agente eligiera explorar la acción tres (3) en este estado, iría hacia el oeste y chocaría contra una pared. El código fuente ha hecho imposible mover el taxi a través de una pared, por lo que si el taxi elige esa acción, seguirá acumulando -1 penalizaciones, lo que afecta la recompensa a largo plazo."]},{"cell_type":"markdown","id":"2bc0668d","metadata":{"id":"2bc0668d"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","id":"0600f9b6","metadata":{"id":"0600f9b6"},"source":["### Paso 1. Resolvemos el problema sin aprendizaje por refuerzo\n","\n","Veamos qué pasaría si intentamos utilizar la fuerza bruta para resolver el problema sin RL.\n","Dado que tenemos nuestra tabla P para las recompensas predeterminadas en cada estado, podemos intentar que nuestro taxi navegue solo con eso.\n","Crearemos un bucle infinito que se ejecutará hasta que un pasajero llegue a un destino (un episodio), o en otras palabras, cuando la recompensa recibida sea 20.\n","\n","El método env.action_space.sample () selecciona automáticamente una acción aleatoria del conjunto de todas las acciones posibles.\n"]},{"cell_type":"code","execution_count":null,"id":"c71199c9","metadata":{"id":"c71199c9","outputId":"ae1af5e2-8aac-4543-abc4-d99ecf6a0172"},"outputs":[{"name":"stdout","output_type":"stream","text":["Timesteps taken: 304\n","Penalties incurred: 92\n"]}],"source":["env.s = 328  # el estado de la imagen\n","\n","epochs = 0\n","penalties, reward = 0, 0\n","\n","frames = [] # for animation\n","\n","done = False\n","\n","while not done:\n","    action = env.action_space.sample()\n","    state, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","    \n","    # Put each rendered frame into dict for animation\n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","        }\n","    )\n","\n","    epochs += 1\n","    \n","    \n","print(\"Timesteps taken: {}\".format(epochs))\n","print(\"Penalties incurred: {}\".format(penalties))\n"]},{"cell_type":"code","execution_count":null,"id":"c47542ab","metadata":{"collapsed":true,"id":"c47542ab"},"outputs":[],"source":["from IPython.display import clear_output\n","from time import sleep\n","\n","def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        #print(frame['frame'].getvalue())\n","        print(frame['frame'])\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.1)"]},{"cell_type":"code","execution_count":null,"id":"2931762f","metadata":{"id":"2931762f","outputId":"e7b452c0-dbf5-4fb7-bd66-fa249088b65b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |B: |\n","+---------+\n","  (Dropoff)\n","\n","Timestep: 304\n","State: 0\n","Action: 5\n","Reward: 20\n"]}],"source":["print_frames(frames)"]},{"cell_type":"markdown","id":"0ca3830d","metadata":{"id":"0ca3830d"},"source":["El agente va a ciegas y utiliza miles de pasos de tiempo y realiza muchos drop-offs incorrectos para entregar un solo pasajero al destino correcto (cuando acierte).  \n","\n","Dejamos como ejercicio opcional la resolución de este problema con cualquiera de los algoritmos de búsqueda de AIMA definiendo la clase Problem usando la codificación numérica de los estados de este entorno y las 5 acciones que hemos visto haciendo una llamada a next_state, reward, done, info = env.step(action) \n","\n","Vamos a resolverlo aquí con Aprendizaje por refuerzo que empieza haciendo ciclos a ciegas y aprende de la experiencia pasada.  Cuando \"por casualidad\" acertamos el agente guarda en la memoria (en forma de recompensa) qué acción fue la mejor para cada estado. Así en el futuro elegirá esa acción. "]},{"cell_type":"code","execution_count":null,"id":"2051cdce","metadata":{"id":"2051cdce","outputId":"76101549-a007-485e-b63c-a8717205d153"},"outputs":[{"name":"stdout","output_type":"stream","text":["State: 328\n","+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| |\u001b[43m \u001b[0m: | : |\n","|\u001b[34;1mY\u001b[0m| : |B: |\n","+---------+\n","\n"]}],"source":["import gym\n","env = gym.make(\"Taxi-v3\").env\n","# Estado inicial aleatorio\n","env.reset() # reset environment to a new, random state   EStado inicial aleatorio \n","# O estado inicial establecido \n","state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n","print(\"State:\", state)\n","env.s = state\n","env.render()"]},{"cell_type":"code","execution_count":null,"id":"dd0d49ed","metadata":{"id":"dd0d49ed","outputId":"3e4beba6-3f84-4b1e-e313-7cbc3c1fa77d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n","+---------+\n","\n"]}],"source":["env.s = 499\n","env.render()\n","# cuadrado amarillo: taxi sin pasajero and green with a passenger.\n","# R, G, Y, B are the possible pickup and destination locations. \n","#The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."]},{"cell_type":"code","execution_count":null,"id":"f23c5468","metadata":{"id":"f23c5468","outputId":"e2f1f7f0-7c77-4c6f-8bd2-26e1d1faeafe"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n","+---------+\n","\n","State: 328\n","Action: 0\n"]}],"source":["state = env.encode(3, 1, 2, 0)     # 3,1 son as coordenadas del taxi.  \n","# 2 y 0 son las posiciones del pasajero y dejada del pasajero que estan numeradas de 0..4\n","#  passenger index, destination index  (creo que para indicar que esta en el taxi se usa tambien .. mirar.)\n","\n","action = env.action_space.sample()    # genera una acción aleatoria\n","# Recordamos las acciones: 0 = south; 1 = north; 2 = east; 3 = west; 4 = pickup; 5 = dropoff\n","env.render()\n","next_state, reward, done, info = env.step(action) \n","print(\"State:\", state)\n","print(\"Action:\", action)\n"]},{"cell_type":"code","execution_count":null,"id":"f40c3655","metadata":{"id":"f40c3655","outputId":"1bf0f5f0-f1f0-4bf1-acf9-3f0afb15af7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Next State: 499\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n","+---------+\n","  (South)\n"]}],"source":["#next_state = env.step(action)[0] \n","print(\"Next State:\", next_state)\n","env.s = next_state\n","env.render()"]},{"cell_type":"code","execution_count":null,"id":"f9e14d69","metadata":{"id":"f9e14d69","outputId":"6245b6cc-d504-469e-f3e7-3a740e22e545"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n","+---------+\n","  (South)\n","State: 328\n","Next Action: 5\n"]}],"source":["action = 5   \n","# Recordamos las acciones: 0 = south; 1 = north; 2 = east; 3 = west; 4 = pickup; 5 = dropoff\n","env.render()\n","next_state, reward, done, info = env.step(action) \n","print(\"State:\", state)\n","print(\"Next Action:\", action)"]},{"cell_type":"code","execution_count":null,"id":"876e23b2","metadata":{"id":"876e23b2","outputId":"7a9ff685-33ce-4454-dcb1-53a503bfa3c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Next State: 499\n","+---------+\n","|R: | : :G|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n","+---------+\n","  (Dropoff)\n"]}],"source":["print(\"Next State:\", next_state)\n","env.s = next_state\n","env.render()\n","# como hacemos dropoff de un taxi vacio no hace nada"]},{"cell_type":"code","execution_count":null,"id":"44acf56a","metadata":{"id":"44acf56a","outputId":"9afa9a77-61f8-4959-db24-6d787186ffd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","|\u001b[43m \u001b[0m| : | : |\n","|Y| : |B: |\n","+---------+\n","  (Dropoff)\n","State: 300\n","Next Action: 3\n","Next State: 300\n"]}],"source":["env.s = 300\n","state = env.s\n","next_state = 0\n","action = 3   \n","env.render()\n","next_state, reward, done, info = env.step(action) \n","print(\"State:\", state)\n","print(\"Next Action:\", action)\n","print(\"Next State:\", next_state)"]},{"cell_type":"code","execution_count":null,"id":"976b8f41","metadata":{"id":"976b8f41","outputId":"b2c0082f-3843-4144-9fad-a8ba623b3d30"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","|\u001b[43m \u001b[0m| : | : |\n","|Y| : |B: |\n","+---------+\n","  (West)\n"]}],"source":["env.s = next_state\n","env.render()"]},{"cell_type":"markdown","id":"031899c9","metadata":{"id":"031899c9"},"source":["### Aprendizaje por refuerzo\n","\n","Vamos a utilizar el algoritmo de Q-learning que hemos visto en clase y que le dará a nuestro agente algo de memoria.\n","Básicamente, Q-learning permite al agente utilizar las recompensas del entorno para aprender, con el tiempo, la mejor acción a realizar en un estado determinado.\n","En nuestro entorno de Taxi, tenemos la tabla de recompensas, P, de la que el agente aprenderá. Lo hace buscando recibir una recompensa por realizar una acción en el estado actual y luego actualizar un valor Q para recordar si esa acción fue beneficiosa.\n","Los valores almacenados en la tabla Q se denominan valores Q y se asignan a una combinación (estado, acción).\n","Un valor Q para una combinación de acción de estado particular es representativo de la \"calidad\" de una acción tomada desde ese estado. Mejores valores Q implican mejores posibilidades de obtener mayores recompensas.\n","Por ejemplo, si el taxi se enfrenta a un estado que incluye a un pasajero en su ubicación actual, es muy probable que el valor Q para la recogida sea más alto en comparación con otras acciones, como la bajada o el norte.\n","Los valores Q se inicializan a un valor arbitrario y, a medida que el agente se expone al entorno y recibe diferentes recompensas al ejecutar diferentes acciones, los valores Q se actualizan mediante la ecuación:\n","\n","Q (estado, acción) ← (1 − α) Q (estado, acción) + α (recompensa + γ maxa Q (siguiente estado, todas las acciones))\n","\n","Dónde:\n","- α (alfa) es la tasa de aprendizaje (0 <α≤1) - Al igual que en los entornos de aprendizaje supervisado, αα es la medida en que nuestros valores Q se actualizan en cada iteración.\n","- γ (gamma) es el factor de descuento (0≤γ≤1) - determina cuánta importancia queremos dar a las recompensas futuras. Un valor alto para el factor de descuento (cercano a 1) captura la recompensa efectiva a largo plazo, mientras que un factor de descuento de 0 hace que nuestro agente considere solo la recompensa inmediata.\n","\n","En la fórmula anterior estamos asignando (←), o actualizando, el valor Q del estado actual y la acción del agente tomando primero un peso (1 − α) del antiguo valor Q y luego agregando el valor aprendido. El valor aprendido es una combinación de la recompensa por realizar la acción actual en el estado actual y la recompensa máxima descontada del siguiente estado en el que estaremos una vez que realicemos la acción actual.\n","Básicamente, estamos aprendiendo la acción adecuada a tomar en el estado actual al observar la recompensa por la combinación estado / acción actual y las recompensas máximas para el siguiente estado. Esto eventualmente hará que nuestro taxi considere la ruta con las mejores recompensas.\n","El valor Q de un par estado-acción es la suma de la recompensa instantánea y la recompensa futura descontada (del estado resultante). La forma en que almacenamos los valores Q para cada estado y acción es a través de una tabla Q\n","\n","La tabla Q es una matriz donde tenemos una fila para cada estado (500) y una columna para cada acción (6). Primero se inicializa a 0 y luego los valores se actualizan después del entrenamiento. Tenga en cuenta que la Q-table tiene las mismas dimensiones que la mesa de recompensas, pero tiene un propósito completamente diferente.\n","\n","En la siguiente figura los valores de la Q-Table se inicializan a cero y se van actualizando durante el aprendizaje.  Los valores optimizan el recorrido del agente a traves del entorno buscando las máximas recompensas. "]},{"cell_type":"markdown","id":"94618359","metadata":{"id":"94618359"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","id":"e78daf52","metadata":{"id":"e78daf52"},"source":["### Resumen del proceso de Q-Learning\n","\n","    • Inicializar la tabla Q a todo ceros.\n","    • Comenzar a explorar acciones: para cada estado, seleccione cualquiera de las posibles acciones para el estado actual (S).\n","    • Ir al siguiente estado (S ') como resultado de esa acción (a).\n","    • Para todas las acciones posibles del estado (S '), seleccione la que tenga el valor Q más alto.\n","    • Actualizar los valores de la tabla Q utilizando la ecuación.\n","    • Establecer el siguiente estado como el estado actual.\n","    • Si se alcanza el estado objetivo, finalizar y repetir el proceso.\n","\n","### Explotación de valores aprendidos\n","\n","Después de una fase de exploración aleatoria de acciones, los valores Q tienden a converger sirviendo a nuestro agente como una función de valor de acción que puede explotar para elegir la acción mejor para un estado dado.\n","Existe una compensación entre exploración (elegir una acción aleatoria) y explotación (elegir acciones basadas en valores Q ya aprendidos). Queremos evitar que la acción siga siempre la misma ruta y posiblemente se sobreajuste, por lo que introduciremos otro parámetro llamado ϵ \"épsilon\" para atender esto durante el entrenamiento.\n","En lugar de simplemente seleccionar la acción de valor Q mejor aprendida, a veces preferimos explorar más el espacio de acción. Un valor de épsilon más bajo da como resultado episodios con más penalizaciones (en promedio), lo cual es obvio porque estamos explorando y tomando decisiones al azar."]},{"cell_type":"code","execution_count":null,"id":"469bbf55","metadata":{"id":"469bbf55"},"outputs":[],"source":["# Primero se inicializa la Q-table a 500×6500×6 matrix of zeros:\n","\n","import numpy as np\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n"]},{"cell_type":"markdown","id":"75e4110e","metadata":{"id":"75e4110e"},"source":["Ejecutar el aprendizaje es un proceso lento pero se realiza una única vez. \n","Una vez entrenado, podemos resolver cualquier problema de este entorno simplemente consultando la tabla y eligiendo la acción que maximiza la recompensa en cada paso (explotación). \n","El algoritmo de entrenamiento que actualizará esta Q-table a medida que el agente explora el entorno durante miles de episodios. \n","\n","En la primera parte del bucle (while not done) decidimos si elegir una acción aleatoria o explotar los valores Q ya calculados. Esto se hace simplemente usando el valor épsilon y comparándolo con la función random.uniform (0, 1), que devuelve un número arbitrario entre 0 y 1.\n","Ejecutamos la acción elegida en el entorno para obtener el next_state y la recompensa por realizar la acción. Después de eso, calculamos el valor Q máximo para las acciones correspondientes al next_state, y con eso, podemos actualizar fácilmente nuestro valor Q al new_q_value:"]},{"cell_type":"code","execution_count":null,"id":"b826f253","metadata":{"id":"b826f253","outputId":"38460b81-72f0-4f7d-f54a-eb0bc9e1f821"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 100000\n","Training finished.\n","\n","Wall time: 26.9 s\n"]}],"source":["%%time\n","\"\"\"Training the agent\"\"\"\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action) \n","        \n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","        \n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")\n"]},{"cell_type":"code","execution_count":null,"id":"b419a0aa","metadata":{"id":"b419a0aa","outputId":"4f386c82-ed7a-4949-b309-ee6f52eb4a44"},"outputs":[{"data":{"text/plain":["array([ -2.41837066,  -2.27325184,  -2.41837066,  -2.3639511 ,\n","       -11.3639511 , -11.3639511 ])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# La tabla Q ha cambiado despues de 100000 episodios. Vamos a ver cuales son los Q-values aprendidos en el estado de ejemplo. \n","q_table[328]"]},{"cell_type":"markdown","id":"1a61058e","metadata":{"id":"1a61058e"},"source":["El valor máximo de Q es \"norte\" (-1,971), por lo que parece que Q-learning ha aprendido efectivamente la mejor acción a realizar en el estado de la imagen."]},{"cell_type":"markdown","id":"5080c23d","metadata":{"id":"5080c23d"},"source":["### Evaluar el comportamiento del agente despues del proceso de Q-learning \n","\n","Para evaluar el comportamiento de nuestro agente no necesitamos explorar más acciones. En el comportamiento del agente ahora la siguiente acción siempre se selecciona utilizando el mejor valor Q:"]},{"cell_type":"code","execution_count":null,"id":"cdca532b","metadata":{"id":"cdca532b","outputId":"6dca0a87-c289-484c-eb12-7c42c5f79c00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results after 100 episodes:\n","Average timesteps per episode: 13.15\n","Average penalties per episode: 0.0\n"]}],"source":["total_epochs, total_penalties = 0, 0\n","episodes = 100\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")\n"]},{"cell_type":"markdown","id":"b83b7224","metadata":{"id":"b83b7224"},"source":["Podemos ver en la evaluación que el comportamiento del agente ha mejorado significativamente y no hay penalizaciones, lo que significa que realizó las acciones correctas de recogida / devolución con 100 pasajeros diferentes.\n","\n","Comparando los dos agentes vemos como aunque con Q-learning el agente comete errores inicialmente durante la exploración, una vez que ha explorado lo suficiente (visto la mayoría de los estados), puede actuar sabiamente maximizando las recompensas haciendo movimientos inteligentes. \n","\n","Veamos cuánto mejor es nuestra solución de Q-learning en comparación con el agente que realiza movimientos aleatorios.\n","\n","Evaluamos a nuestros agentes de acuerdo con las siguientes métricas,\n","\n","    • Número medio de penalizaciones por episodio: Cuanto menor sea el número, mejor será el desempeño de nuestro agente. Idealmente, nos gustaría que esta métrica fuera cero o muy cercana a cero.\n","\n","    • Número promedio de pasos de tiempo por viaje: también queremos un número pequeño de pasos de tiempo por episodio, ya que queremos que nuestro agente dé pasos mínimos (es decir, el camino más corto) para llegar al destino.\n","\n","    • Promedio de recompensas por movimiento: cuanto mayor sea la recompensa, significa que el agente está haciendo lo correcto. Es por eso que decidir las recompensas es una parte crucial del aprendizaje por refuerzo. En nuestro caso, dado que tanto los tiempos como las penalizaciones se recompensan negativamente, una recompensa promedio más alta significaría que el agente llega al destino lo más rápido posible con la menor cantidad de penalizaciones.\n","\n","\n","    Average rewards per move\t-3.9012092102214075\t0.6962843295638126\n","    Average number of penalties per episode\t920.45\t0.0\n","    Average number of timesteps per trip\t2848.14\t12.38\n","\n","Estas métricas se calcularon en más de 100 episodios. \n","Y como muestran los resultados, nuestro agente de Q-learning tiene un buen comportamiento. \n"]},{"cell_type":"markdown","id":"ec7eb782","metadata":{"id":"ec7eb782"},"source":["### Hiperparámetros y optimizaciones\n","\n","Los valores de `alpha`,` gamma` y `epsilon` que hemos utilizado han sido elegidos por intuición, prueba y error pero hay mejores formas de obtener buenos valores. Idealmente, los tres deberían disminuir con el tiempo porque a medida que el agente continúa aprendiendo, en realidad construye antecedentes más válidos y duraderos;\n","\n","    • α: (la tasa de aprendizaje) debería disminuir a medida que continúa adquiriendo una base de conocimientos cada vez mayor.\n","    • γ: a medida que se acerca cada vez más al valor límite, su preferencia por la recompensa a corto plazo debería aumentar, ya que no estará el tiempo suficiente para obtener la recompensa a largo plazo, lo que significa que su gamma debería disminuir.\n","    • ϵ: a medida que desarrollamos nuestra estrategia, tenemos menos necesidad de exploración y más explotación para obtener más utilidad de nuestra política, por lo que a medida que aumentan los ensayos, épsilon debería disminuir.\n","\n","Una forma de obtener la combinación correcta de valores de hiperparámetros sería usar optimización local con algoritmos genéticos. "]},{"cell_type":"markdown","id":"00e31dbe","metadata":{"id":"00e31dbe"},"source":["Q-learning es uno de los algoritmos de aprendizaje por refuerzo más fáciles. Sin embargo, el problema con la obtención de Q es que, una vez que el número de estados en el entorno es muy alto, se vuelve difícil implementarlos con la tabla Q, ya que el tamaño sería muy, muy grande. Por eso se utilizan redes neuronales profundas en lugar de Q-table (Deep Reinforcement Learning). La red neuronal recibe información de estado y acciones en la capa de entrada y aprende a generar la acción correcta a lo largo del tiempo. Las técnicas de aprendizaje profundo (como las redes neuronales convolucionales) también se utilizan para interpretar los píxeles en la pantalla y extraer información del juego (como puntuaciones), y luego dejar que el agente controle el juego."]},{"cell_type":"markdown","id":"5e22bac2","metadata":{"id":"5e22bac2"},"source":["### Ejercicios   \n","\n","#### Ejercicio 1. \n","Hemos realizado aprendizaje. Utiliza los valores Q aprendidos para solucionar otro problema, es decir, cambiando el estado inicial/objetivo  \n","\n","\n","#### Ejercicio 2.\n","\n","En el problema del Taxi resuelto con Q-Learning se pide experimentar con distintos estados iniciales y distintos valores de los hiperparámetros y con distinto números de episodios de aprendizaje y comentar los resultados obtenidos. Observa el comportamiento del agente con los valores límite de los hiperparámetros.\n","\n","Comenta de forma razonada las conclusiones obtenidas de los distintos procesos de aprendizaje. \n","Evalua y compara los agentes respecto a las métricas dadas \n","\n","#### Ejercicio 3.\n","\n","Razona cómo se comportaría el agente con Q-learning si lo comparamos con un agente que resuelve el problema con búsqueda en el espacio de estados. Indica las ventajas e inconvenientes de las dos aproximaciones. \n","\n"]},{"cell_type":"code","execution_count":null,"id":"4f30523a","metadata":{"id":"4f30523a","outputId":"1e3959d9-e528-4e6e-cd9e-f6966e5bca16"},"outputs":[{"name":"stdout","output_type":"stream","text":["penalties: 0\n","timesteps: 55\n"]}],"source":["# Ejercicio 1\n","\n","estado = env.encode(2, 2, 3, 1)\n","env.s = estado\n","\n","total_epochs, total_penalties = 0, 0\n","epochs, penalties, reward = 0, 0, 0\n","done = False\n","\n","while not done:\n","    action = np.argmax(q_table[estado])\n","    estado, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","    epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"penalties: {total_penalties}\")\n","print(f\"timesteps: {total_epochs}\")"]},{"cell_type":"code","execution_count":null,"id":"4faa096b","metadata":{"collapsed":true,"id":"4faa096b"},"outputs":[],"source":["# Ejercicio 2"]},{"cell_type":"code","execution_count":null,"id":"237e91c7","metadata":{"collapsed":true,"id":"237e91c7"},"outputs":[],"source":["q_table = np.zeros([env.observation_space.n, env.action_space.n])"]},{"cell_type":"code","execution_count":null,"id":"30a25073","metadata":{"id":"30a25073","outputId":"f1738548-540b-4f0c-9cd1-8452f34ae438"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 1000\n","Training finished.\n","\n","Wall time: 43.6 s\n"]}],"source":["%%time\n","\"\"\"Training the agent\"\"\"\n","\n","state = env.encode(2, 2, 3, 1)\n","env.s = estado\n","\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 1001):\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action) \n","        \n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","        \n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"code","execution_count":null,"id":"ed152986","metadata":{"id":"ed152986","outputId":"44030405-cacd-4cbd-fe4e-b5dc1cd6fc17"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results after 50000 episodes:\n","Average timesteps per episode: 13.0669\n","Average penalties per episode: 0.0\n"]}],"source":["total_epochs, total_penalties = 0, 0\n","episodes = 50000\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")\n"]},{"cell_type":"markdown","id":"5fae9ba3","metadata":{"id":"5fae9ba3"},"source":["     Probando con 50 000 episodios\n","\n","    Episodes   alpha    gamma   epsilon  avg timesteps  avg penalties\n","    100 000     0.1      0.6     0.1         13.149         0.0\n","    50 000      0.1      0.6     0.1         13.218         0.0\n","    \n","    1 000       0.1      0.6     1           13.132         0.0\n","    500         0.1      0.6     0.1         13.047         0.0\n","    \n","    1 000 000   0.3      0.6     0.1         13.011         0.0\n","    100 000     0.3      0.6     0.1         12.975         0.0\n","    50 000      0.3      0.6     0.1         13.109         0.0\n","    \n","    100 000     0.7      0.6     0.1         13.042         0.0\n","    50 000      0.7      0.6     0.1         13.069         0.0\n","    \n","    100 000     0.3      0.8     0.1         13.149         0.0\n","    50 000      0.3      0.8     0.1         13.005         0.0\n","    \n","    100 000     0.3      0.6     0.3         13.142         0.0\n","    50 000      0.3      0.6     0.3         12.932         0.0\n","    \n","    50 000      0.8      0.3     0.9         13.115         0.0\n","    10 000      0.8      0.3     0.9         13.039         0.0\n","    1 000       0.8      0.3     0.9         13.165         0.0\n","    \n","    "]},{"cell_type":"markdown","id":"62a5cd4e","metadata":{"id":"62a5cd4e"},"source":["CONCLUSIONES \n","\n","- Queda muy parecido, seguramente por un número de episodios de prueba insuficiente."]},{"cell_type":"code","execution_count":null,"id":"9d6f1c89","metadata":{"id":"9d6f1c89"},"outputs":[],"source":["# Ejercicio 3"]},{"cell_type":"markdown","id":"321ae62d","metadata":{"id":"321ae62d"},"source":["    \n"," - El agente con Q-learning resuelve cada episodio con mucha mayor velocidad y eficacia una vez que está bien entrenado. Sin embargo, el tiempo de entrenamiento puede llegar a ser prolongado y se debe estudiar qué valores para los hiperparámetros alpha gamma y épsilon son adecuados. Por otro lado, se tiene la desventaja de que si se cambia el problema se deberá calcular la tabla q de nuevo, lo que lleva asociado un elevado coste en tiempo.\n"," - Por el contrario, si se usa un agente de búsqueda en el espacio de soluciones, este será más ágil a la hora de realizar cambios en el problema pues cada cálculo es independiente. Como contrapartida se tiene que se recorren más estados y el tiempo de resolución de cada caso es mayor en general.\n"," - En conclusión, de forma general será más efectivo usar un agente con Q-learning cuando se vayan a resolver muchos casos de un mismo problema porque amortizaremos el gasto en tiempo de calcular la tabla q y preferiremos usar un agente de búsqueda en caso contrario."]},{"cell_type":"markdown","id":"6bcbb060","metadata":{"id":"6bcbb060"},"source":["    "]},{"cell_type":"markdown","id":"102a3c95","metadata":{"id":"102a3c95"},"source":["### Parte 2. Cart Pole"]},{"cell_type":"markdown","id":"34e4e4ee","metadata":{"id":"34e4e4ee"},"source":["Realizar el mismo proceso de entrenamiento con QLearning para otro entorno de OpenAI\n","https://gym.openai.com/envs/CartPole-v1/  \n","\n","Como lo has usado en el ejercicio anterior no necesitas volver a instalar la librería Gym que ya incluye el entorno CartPole."]},{"cell_type":"code","execution_count":null,"id":"483ac989","metadata":{"collapsed":true,"id":"483ac989"},"outputs":[],"source":["## pip install gym[all] \n","## para instalar gym con todas sus dependencias"]},{"cell_type":"markdown","id":"633ef42a","metadata":{"id":"633ef42a"},"source":["Un poste está unido por una articulación no accionada a un carro, que se mueve a lo largo de una pista sin fricción. El sistema se controla aplicando una fuerza de +1 o -1 al carro. El péndulo comienza en posición vertical y el objetivo es evitar que se caiga. Se proporciona una recompensa de +1 por cada paso de tiempo que el poste permanece en posición vertical. El episodio termina cuando el poste está a más de 15 grados de la vertical o el carro se mueve más de 2.4 unidades desde el centro."]},{"cell_type":"code","execution_count":null,"id":"b75d2fe7","metadata":{"id":"b75d2fe7","outputId":"ca240c4d-3755-4da5-bd45-5d896ef21471"},"outputs":[{"ename":"ImportError","evalue":"\n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    ","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyglet'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m<ipython-input-14-9ffdc2432342>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m     19\u001b[0m     \u001b[0mCannot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mImportError\u001b[0m: \n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    "]}],"source":["# Generamos el entorno CartPole v1 y realizamos acciones aleatorias \n","\n","env = gym.make('CartPole-v1')\n","for i_episode in range(20):\n","    observation = env.reset()\n","    for t in range(100):\n","        env.render()\n","        print(observation)\n","        action = env.action_space.sample()\n","        observation, reward, done, info = env.step(action)\n","        if done:\n","            print(\"Episode finished after {} timesteps\".format(t+1))\n","            break\n","env.close()"]},{"cell_type":"markdown","id":"808d9f3d","metadata":{"id":"808d9f3d"},"source":["### Ejercicios.\n","\n","#### Ejercicio 4. \n","Se pide realizar un agente que aprenda a resolver el problema del CartPole usando Q-Learning. Comenta el resultado obtenido y realiza pruebas (como en el ejercicio 2) para comprobar el comportamiento con distintos valores de los hiperparámetros. \n","\n","En el ejercicio 4 el entorno propuesto es Cart Pole pero puedes usar cualquiera de los incluidos en OPEN AI, por ejemplo, también es sencillo el entorno FrozenLake-v0  https://gym.openai.com/envs/FrozenLake-v0/ \n","En este entorno el agente controla el movimiento de un personaje en un mundo de rejilla. Algunas baldosas son transitables (walkable) y otras hacen que el agente caiga al agua. La dirección de movimiento del agente es incierta y solo depende parcialmente de la dirección elegida. \n","La recompensa se obtiene cuando el agente llega a traves de un camino transitable a una casilla objetivo. "]},{"cell_type":"code","execution_count":null,"id":"bac91b9c","metadata":{"id":"bac91b9c","outputId":"ade04547-5ea2-44fb-f344-c4e9686fbdf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Down)\n","SFFF\n","FHFH\n","\u001b[41mF\u001b[0mFFH\n","HFFG\n","8\n","Episode finished after 3 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Down)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Left)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Up)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Left)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Down)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","Episode finished after 23 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","Episode finished after 3 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","Episode finished after 3 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Right)\n","SFFF\n","FHFH\n","\u001b[41mF\u001b[0mFFH\n","HFFG\n","8\n","Episode finished after 3 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Left)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","Episode finished after 18 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Right)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Up)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Down)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","Episode finished after 11 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","SFFF\n","FH\u001b[41mF\u001b[0mH\n","FFFH\n","HFFG\n","6\n","Episode finished after 4 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","Episode finished after 5 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","Episode finished after 2 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","Episode finished after 2 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Left)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Down)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SFFF\n","FH\u001b[41mF\u001b[0mH\n","FFFH\n","HFFG\n","6\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SFFF\n","FH\u001b[41mF\u001b[0mH\n","FFFH\n","HFFG\n","6\n","  (Left)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","SFFF\n","FH\u001b[41mF\u001b[0mH\n","FFFH\n","HFFG\n","6\n","  (Right)\n","SFFF\n","FHFH\n","FF\u001b[41mF\u001b[0mH\n","HFFG\n","10\n","  (Right)\n","SFFF\n","FH\u001b[41mF\u001b[0mH\n","FFFH\n","HFFG\n","6\n","Episode finished after 22 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Right)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","Episode finished after 6 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","Episode finished after 3 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","Episode finished after 5 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Up)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Left)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Left)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","Episode finished after 17 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Left)\n","SFFF\n","FH\u001b[41mF\u001b[0mH\n","FFFH\n","HFFG\n","6\n","Episode finished after 6 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Down)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Right)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Down)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Left)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Left)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","Episode finished after 9 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Right)\n","SFF\u001b[41mF\u001b[0m\n","FHFH\n","FFFH\n","HFFG\n","3\n","  (Up)\n","SF\u001b[41mF\u001b[0mF\n","FHFH\n","FFFH\n","HFFG\n","2\n","  (Down)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Right)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","Episode finished after 17 timesteps\n","\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","4\n","  (Left)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","0\n","  (Up)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","  (Right)\n","S\u001b[41mF\u001b[0mFF\n","FHFH\n","FFFH\n","HFFG\n","1\n","Episode finished after 6 timesteps\n"]}],"source":["env = gym.make('FrozenLake-v1')\n","for i_episode in range(20):\n","    observation = env.reset()\n","    for t in range(100):\n","        env.render()\n","        print(observation)\n","        action = env.action_space.sample()\n","        observation, reward, done, info = env.step(action)\n","        if done:\n","            print(\"Episode finished after {} timesteps\".format(t+1))\n","            break\n","env.close()"]},{"cell_type":"code","execution_count":null,"id":"343ab01e","metadata":{"id":"343ab01e","outputId":"0e99697e-25ef-426a-e2dd-6fbfcd68ba13"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 10000\n","Training finished.\n","\n","Wall time: 3.51 s\n"]}],"source":["%%time\n","\"\"\"Training the agent\"\"\"\n","\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hyperparameters\n","alpha = 0.3\n","gamma = 0.9\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_success = []\n","\n","for i in range(1, 10001):\n","    state = env.reset()\n","\n","    epochs, success, reward, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action) \n","        \n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","        \n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == 1:\n","            success += 1\n","\n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"]},{"cell_type":"code","execution_count":null,"id":"30fb1859","metadata":{"id":"30fb1859","outputId":"05b01054-c8c5-485d-e86d-639a9d5a7f3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results after 100000 episodes:\n","Average timesteps per episode: 26.91776\n","Average success per episode: 0.38782\n"]}],"source":["total_epochs, total_success = 0, 0\n","episodes = 100000\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, success, reward = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","\n","        if reward == 1:\n","            success += 1\n","\n","        epochs += 1\n","\n","    total_success += success\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average success per episode: {total_success / episodes}\")\n"]},{"cell_type":"markdown","id":"f15eb3f9","metadata":{"id":"f15eb3f9"},"source":["            Probando con 100 000 episodios\n","\n","     Comb.  Episodes   alpha    gamma   epsilon  sucess_rate (0-1)\n","\n","         1  1 000      0.1      0.6     0.1      0.0\n","            10 000     0.1      0.6     0.1      0.119\n","            100 000    0.1      0.6     0.1      0.167\n","\n","         2  10 000     0.1      0.2     0.1      0.055\n","            50 000     0.1      0.2     0.1      0.108\n","            100 000    0.1      0.2     0.1      0.108\n","\n","         3  10 000     0.9      0.6     0.1      0.101\n","            100 000    0.9      0.6     0.1      0.146\n","\n","         4  10 000     0.1      0.6     0.9      0.567 Volviendo a entrenar da 0.138\n","            100 000    0.1      0.6     0.9      0.116\n","\n","         5  100 000    0.1      0.6     1.0      0.251 Volviendo a entrenar da 0.345\n","\n","         6  10 000     0.1      0.9     0.1      0.0   Volviendo a entrenar 0.601, 0.0 y 0.505(??)\n","            100 000    0.1      0.9     0.1      0.203 Volviendo a entrenar 0.711 (QUÉ LOCURA)\n","\n","         7  10 000     0.5      0.9     0.1      0.0   Otra vez da lo mismo y otra da 0.143  \n","            100 000    0.5      0.9     0.1      0.510\n","\n","         8  10 000     0.3      0.9     0.1      0.0   Otra vez igual y otra vez 0.388\n","            50 000     0.3      0.9     0.1      0.253\n","            100 000    0.3      0.9     0.1      0.580\n","            1 000 000  0.3      0.9     0.1      0.448\n","\n","         9  100 000    0.4      0.9     0.1      0.443\n","\n","         10 50 000     0.3      0.9     0.4      0.132\n","            100 000    0.3      0.9     0.4      0.328"]},{"cell_type":"markdown","id":"f981eb2e","metadata":{"id":"f981eb2e"},"source":["CONCLUSIONES:\n","\n","- Lo primero que podemos observar para la mayoría de combincaciones de hiperparámetros es que un mayor número de episodios en la fase de entrenamiento resulta por lo general en un mayor porcentaje de éxito a la hora de ejecutar el algoritmo.\n","\n","- Otra observación general es que valores extremos de cualquiera de los parámetros (cercanos a 0 o a 1) no suelen dar buenos resultados. Esto lo observamos en las 6 primeras combianciones, en las que por lo general no se pasa de un 25% de éxito.\n","\n","- Parámetro Gamma (tasa de descuento): Debido a que es un problema que solo recibe una recompensa y además positiva cuando finaliza con éxito, supusimos que tener \"más en cuenta\" la recompensa a largo plazo que la inmediata (valor de gamma alto) positivo daría buenos resultados. Podemos observar que en combinaciones de parámetros en la que la gamma es más cercana a 0 tenemos un porcentaje de éxito más bajo (comb.2 gamma=0.2 éxito=10.8% con 100 000 episodios) que en otras con la gamma más cercana a 1 (comb.1 gamma=0.6 éxito=16.7% con 100 000 episodios, ó comb.6 gamma=0.9 éxito= 20.3%).\n","\n","- Parámetro Epsilon (Aleatoriedad): Con valores de epsilon cercanos a 1 como en las combinaciones 4 (epsilon=0.9) y 5 (epsilon=1) podemos observar como para entrenamientos de 100 000 episodios obtenemos porcentajes de éxitos bajos (11.6% y 25.1%) ya que no explota lo aprendido nunca, sino que genera aleatoriamente el 100% de las veces. Se pueden dar casos en los que el porcentaje de éxito sea alto ya que las decisiones que va a tomar van a ser basadas en la aleatoriedad y puede resultar que se haya generado aleatoriamente una tabla-q \"más o menos\" buena, pero en general no es el caso. También es importante apuntar que al ser 100% aleatorio da igual el número de veces que entrenemos, (lo que se puede observar en la combinación 4 de parámetros, nos da una tasa de éxito mucho mayor entrenando menos, lo cual es casualidad, resultado de la aleatoriedad).\n","\n","- Parámetro alpha (tasa de aprendizaje): Como se indica al principio observamos que valores extremos dan un porcentaje de éxito menor que valores más intermedios (como en la últimas combinaciones).\n","\n","- Hemos observado que en general da mejores resultados una combinación de una tasa e apredizaje intermedia combinada con una tasa de descuento alta y un epsilon bajo. Es decir, conviene tener en cuenta en la misma medida (más o menos) la nueva experiencia que lo ya aprendido hasta el momento, así como tener en cuenta una recompensa mixta en la que se le da más prioridad a la recompensa a largo plazo que a los refuerzos inmediatos pues en la mayoría de pasos estos últimos no existen. La aleatoridad es necesaria para poder explorar el espacio de soluciones pero no en medida desproporcionada.\n","\n"]},{"cell_type":"markdown","id":"714526ca","metadata":{"id":"714526ca"},"source":["#### Ejercicio 5.  Opcional.   \n","\n","Aplica el algoritmo Q-learning para diseñar un agente que aprenda a resolver alguno de los puzles de la práctica 1 (puzle de 8, jarras, misioneros,..) y comenta el resultado comparandolo con el agente que ya tienes hecho de la práctica anterior que resuelve el problema usando búsqueda en espacio de estados.  \n","Discute claramente las ventajas e inconvenientes con las métricas y resultados obtenidos."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}